{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Student Name:\n",
    "#### Student ID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 7\n",
    "\n",
    "### Mozart Dice Game RNN\n",
    "\n",
    "Instructions: \n",
    "\n",
    "* This notebook is an interactive assignment; please read and follow the instructions in each cell. \n",
    "\n",
    "* Cells that require your input (in the form of code or written response) will have 'Question #' above.\n",
    "\n",
    "* After completing the assignment, please submit this notebook and a copy as a PDF.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 22:07:22.417606: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/r8chen/.local/lib/python3.9/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['svg']\n",
    "from scipy.io import wavfile\n",
    "from numpy.linalg import svd\n",
    "from scipy.stats.mstats import gmean\n",
    "from matplotlib import rcParams\n",
    "import scipy\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import pickle\n",
    "from music21 import converter, instrument, note, chord, stream\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, ReLU, Activation, Lambda, Softmax\n",
    "from keras.layers import BatchNormalization as BatchNorm\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import ModelCheckpoint, ProgbarLogger\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Music with RNN\n",
    "\n",
    "In the next section, you will practice using Keras to create a generative model based on the music of your & your classmates' Mozart Dice Game from Assignment 1. \n",
    "\n",
    "You will be constructing an RNN by filling in some missing lines of code & answering questions about Keras and model performance. \n",
    "\n",
    "The overall goal of this model is to be able to predict the next note of a sequence, given a sequence of 4 notes. (This sequence length of 4 was chosen arbitrarily; please feel free to experiment with this number). \n",
    "\n",
    "First, let's define the RNN model we will use. \n",
    "A base LSTM layer has been included below.\n",
    "\n",
    "##### Question 1 (30 points)\n",
    "\n",
    "Define & compile the rest of the network as follows:\n",
    "\n",
    "The additional layers of your network will be:\n",
    "1. Another LSTM layer, with 512 units of output which drops 3/10 of the units. \n",
    "2. A batch normalization layer.\n",
    "3. A layer which drops 3/10 of the units. \n",
    "4. A fully connected layer with 256 units of output.\n",
    "5. A ReLU activation layer.\n",
    "6. A batch normalization layer.\n",
    "7. A layer which drops 3/10 of the units. \n",
    "8. A fully connected layer with number of units of output equal to the vocabulary space of the input. \n",
    "9. A softmax activation layer which uses a temperature of .6 \n",
    "    (Note, you may need to define this as two separate layers in Keras, using the definition of temperature for softmax). \n",
    "    \n",
    "After creating your network, compile the model with categorical cross entropy loss and an optimizer of your choice. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_network(network_input, n_vocab):\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "        recurrent_dropout=0.3,\n",
    "        return_sequences=True\n",
    "    ))\n",
    "    \n",
    "    ''' Your Code Here '''\n",
    "    model.add(LSTM(\n",
    "        512,\n",
    "        recurrent_dropout = 3/10\n",
    "    ))\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(3/10))\n",
    "    model.add(Dense(256))\n",
    "    model.add(ReLU())\n",
    "    model.add(BatchNorm())\n",
    "    model.add(Dropout(3/10))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Lambda(lambda x: x / .6))\n",
    "    model.add(Softmax())\n",
    "    \n",
    "    model.compile(\n",
    "        loss = 'categorical_crossentropy', \n",
    "        optimizer = 'adam'\n",
    "    )\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will need to structure our input data in a way that makes sense. We can't pass a direct MIDI file to a network, so we must come up with an encoding. Read the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_notes(verbose = False):\n",
    "\n",
    "    notes = []\n",
    "    for file in glob.glob(\"dice_songs/*.mid\"):\n",
    "        midi = converter.parse(file)\n",
    "        if verbose:\n",
    "            print(\"Parsing %s\" % file)\n",
    "        notes_to_parse = None\n",
    "        try: # file has instrument parts\n",
    "            s2 = instrument.partitionByInstrument(midi)\n",
    "            notes_to_parse = s2.parts[0].recurse() \n",
    "        except: # file has notes in a flat structure\n",
    "            notes_to_parse = midi.flat.notes\n",
    "\n",
    "        for element in notes_to_parse:\n",
    "            if isinstance(element, note.Note):\n",
    "                notes.append(str(element.pitch))\n",
    "            elif isinstance(element, chord.Chord):\n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "\n",
    "    pickle.dump(notes, open('notes.p', 'wb'))\n",
    "\n",
    "    return notes\n",
    "\n",
    "\n",
    "def prepare_sequences(notes, n_vocab):\n",
    "    \"\"\" Prepare the sequences used by the Neural Network \"\"\"\n",
    "    sequence_length = 4 \n",
    "\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "\n",
    "    return (network_input, network_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 2 (10 points)\n",
    "\n",
    "How is the data from the MIDI file encoded as input to the network? Be specific in your explanation; make sure you address details such as which data type is used to represent a note in the input layer and how chords are handled, as well as what information is lost by using this encoding. \n",
    "\n",
    "[Hint: Try to print some of the variables to visualize their data.]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The notes are converted to numbers and grouped as chords. The time data is lost."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train the network.\n",
    "\n",
    "##### Question 3 (10 points)\n",
    "\n",
    "Add a line of code to begin the training of the model.\n",
    "Please train for at least 50 epochs (you are welcome to experiment with the duration of training, batch size, and other hyperparameters). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 22:07:29.252838: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-18 22:07:29.835383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22349 MB memory:  -> device: 0, name: TITAN RTX, pci bus id: 0000:60:00.0, compute capability: 7.5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/r8chen/.local/lib/python3.9/site-packages/keras/layers/normalization/batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Epoch 1/1500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-18 22:07:31.052644: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22349 MB memory:  -> device: 0, name: TITAN RTX, pci bus id: 0000:60:00.0, compute capability: 7.5\n",
      "2022-07-18 22:07:31.121668: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4155/4155 [==============================] - 2s 452us/sample - loss: 5.6010\n",
      "Epoch 101/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 3.1977\n",
      "Epoch 201/1500\n",
      "4155/4155 [==============================] - 0s 13us/sample - loss: 2.8583\n",
      "Epoch 301/1500\n",
      "4155/4155 [==============================] - 0s 13us/sample - loss: 2.6389\n",
      "Epoch 401/1500\n",
      "4155/4155 [==============================] - 0s 15us/sample - loss: 2.4767\n",
      "\n",
      "Epoch 500: loss improved from inf to 2.33458, saving model to models/model.hdf5\n",
      "Epoch 501/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 2.3481\n",
      "Epoch 601/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 2.2356\n",
      "Epoch 701/1500\n",
      "4155/4155 [==============================] - 0s 15us/sample - loss: 2.1245\n",
      "Epoch 801/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 2.0593\n",
      "Epoch 901/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 1.9797\n",
      "\n",
      "Epoch 1000: loss improved from 2.33458 to 1.95288, saving model to models/model.hdf5\n",
      "Epoch 1001/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 1.9053\n",
      "Epoch 1101/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 1.8368\n",
      "Epoch 1201/1500\n",
      "4155/4155 [==============================] - 0s 16us/sample - loss: 1.7972\n",
      "Epoch 1301/1500\n",
      "4155/4155 [==============================] - 0s 15us/sample - loss: 1.7323\n",
      "Epoch 1401/1500\n",
      "4155/4155 [==============================] - 0s 14us/sample - loss: 1.6687\n",
      "\n",
      "Epoch 1500: loss improved from 1.95288 to 1.62831, saving model to models/model.hdf5\n"
     ]
    }
   ],
   "source": [
    "checkpoint_filepath = 'models/model.hdf5'\n",
    "\n",
    "def train_network():\n",
    "    \"\"\" Train a Neural Network to generate music \"\"\"\n",
    "    notes = get_notes()\n",
    "\n",
    "    n_vocab = len(set(notes))\n",
    "    \n",
    "    network_input, network_output = prepare_sequences(notes, n_vocab)\n",
    "    \n",
    "    model = create_network(network_input, n_vocab)\n",
    "     \n",
    "    # Your line of code here\n",
    "    global checkpoint_filepath\n",
    "    \n",
    "    class _SelectiveProgbarLogger(ProgbarLogger):\n",
    "        def __init__(self, verbose, epoch_interval, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self.default_verbose = verbose\n",
    "            self.epoch_interval = epoch_interval\n",
    "        \n",
    "        def on_epoch_begin(self, epoch, *args, **kwargs):\n",
    "            self.verbose = (\n",
    "                0 \n",
    "                    if epoch % self.epoch_interval != 0 \n",
    "                    else self.default_verbose\n",
    "            )\n",
    "            super().on_epoch_begin(epoch, *args, **kwargs)\n",
    "    \n",
    "    model.fit(\n",
    "        network_input, network_output, \n",
    "        epochs = 1500, batch_size = 65536,\n",
    "        verbose = 0,\n",
    "        callbacks = [\n",
    "            _SelectiveProgbarLogger(\n",
    "                verbose = 1,\n",
    "                epoch_interval = 100\n",
    "            ),\n",
    "            ModelCheckpoint(\n",
    "                #\"weights2-improvement-{epoch:02d}-{loss:.4f}-bigger.hdf5\",\n",
    "                checkpoint_filepath,\n",
    "                monitor = 'loss',\n",
    "                verbose = 1,\n",
    "                save_best_only = True,\n",
    "                save_freq = 500,\n",
    "                mode = 'min'\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "    \n",
    "_ = train_network()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a trained network to make predictions, it's time to use the network to generate music!\n",
    "\n",
    "##### Question 4 (10 points)\n",
    "\n",
    "To make the predictions, you will need to complete the line in the generate_notes function below.\n",
    "\n",
    "[Hint: what function does Keras use to make predictions?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sequences_prediction(notes, pitchnames, n_vocab):\n",
    "\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "    sequence_length = 4\n",
    "    network_input = []\n",
    "    output = []\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i:i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        output.append(note_to_int[sequence_out])\n",
    "\n",
    "    n_patterns = len(network_input)\n",
    "\n",
    "    # reshape the input into a format compatible with LSTM layers\n",
    "    normalized_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    # normalize input\n",
    "    normalized_input = normalized_input / float(n_vocab)\n",
    "\n",
    "    return (network_input, normalized_input)\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Starts the melody by picking a random sequence from the input as a starting point\n",
    "    start = np.random.randint(0, len(network_input)-1)\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    pattern = network_input[start]\n",
    "    prediction_output = []\n",
    "    \n",
    "    for note_index in range(200):\n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        \n",
    "        ### Complete the line below\n",
    "        prediction = model.predict(\n",
    "            prediction_input,\n",
    "            batch_size = 65536\n",
    "        )\n",
    "\n",
    "        index = np.argmax(prediction)\n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.append(index)\n",
    "        pattern = pattern[1:len(pattern)]\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our model set up, and can create a sequence to use as a query for a prediction of the next note, but we aren't ready to make the predictions since our model does not contain the trained weights!\n",
    "\n",
    "##### Question 5 (10 points)\n",
    "\n",
    "Add a line below to load the weights from your network training. \n",
    "\n",
    "[Hint: What Keras function is used to load weights?]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "notes = pickle.load(open('notes.p', 'rb'))\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "n_vocab = len(set(notes))\n",
    "\n",
    "network_input, normalized_input = prepare_sequences_prediction(notes, pitchnames, n_vocab)\n",
    "model = create_network(normalized_input, n_vocab)\n",
    "\n",
    "### Add a line to load the weights here\n",
    "global checkpoint_filepath\n",
    "model.load_weights(checkpoint_filepath)\n",
    "\n",
    "def generate():\n",
    "    global model, network_input, pitchnames, n_vocab\n",
    "    prediction_output = generate_notes(model, network_input, pitchnames, n_vocab)\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/r8chen/.local/lib/python3.9/site-packages/keras/engine/training_v1.py:2067: UserWarning: `Model.state_updates` will be removed in a future version. This property should not be used in TensorFlow 2.0, as `updates` are applied automatically.\n",
      "  updates=self.state_updates,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv55083'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                \n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv55083');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test_output.mid'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_midi(prediction_output):\n",
    "    offset = 0\n",
    "    output_notes = []\n",
    "    for pattern in prediction_output:\n",
    "        if ('.' in pattern) or pattern.isdigit():\n",
    "            notes_in_chord = pattern.split('.')\n",
    "            notes = []\n",
    "            for current_note in notes_in_chord:\n",
    "                new_note = note.Note(int(current_note))\n",
    "                new_note.storedInstrument = instrument.Piano()\n",
    "                notes.append(new_note)\n",
    "            new_chord = chord.Chord(notes)\n",
    "            new_chord.offset = offset\n",
    "            output_notes.append(new_chord)\n",
    "        else:\n",
    "            new_note = note.Note(pattern)\n",
    "            new_note.offset = offset\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            output_notes.append(new_note)\n",
    "        offset += 0.5\n",
    "    midi_stream = stream.Stream(output_notes)\n",
    "    return midi_stream\n",
    "    \n",
    "midi_stream = create_midi(generate())\n",
    "midi_stream.show('midi')\n",
    "midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 6 (10 points)\n",
    "\n",
    "Listen to your MIDI output. You probably notice that at some point we reach a cycle. Why is this happening? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This happens because we use the same pattern for all iterations. Schematically [\"RNN uses a `for` loop to iterate over the timesteps of a sequence\"](https://www.tensorflow.org/guide/keras/rnn#introduction); therefore the patterns that can be generated from a single pattern are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Question 7 (20 points)\n",
    "\n",
    "The generate_notes function is copied below. Please add your same prediction line from above once more, and then modify the generate_notes function in a way that allows for a non-cyclic composition that still resembles the original input. \n",
    "\n",
    "[Hint: think about what we learned in HW 2 while exploring Markov Chains with the Beatles.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "def generate_notes(model, network_input, pitchnames, n_vocab):\n",
    "    \"\"\" Generate notes from the neural network based on a sequence of notes \"\"\"\n",
    "    # Starts the melody by picking a random sequence from the input as a starting point\n",
    "\n",
    "    int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "    \n",
    "    prediction_output = []\n",
    "\n",
    "    pattern = None\n",
    "    patterns = set()\n",
    "    for _ in range(200):\n",
    "        if (\n",
    "            (pattern is None)\n",
    "                #or (tuple(pattern) in patterns)\n",
    "        ):\n",
    "            pattern = collections.deque(network_input[\n",
    "                np.random.randint(0, len(network_input) - 1)\n",
    "            ])\n",
    "        patterns.add(tuple(pattern))\n",
    "        \n",
    "        prediction_input = np.reshape(pattern, (1, len(pattern), 1))\n",
    "        prediction_input = prediction_input / float(n_vocab)\n",
    "\n",
    "        ### Copy the line below from your above implementation.\n",
    "        prediction = np.reshape(\n",
    "            model.predict(\n",
    "                prediction_input,\n",
    "                batch_size = 65536\n",
    "            ),\n",
    "            -1\n",
    "        )\n",
    "\n",
    "        #index = np.argmax(prediction)\n",
    "        index = np.random.choice(np.arange(len(prediction)), p = prediction)\n",
    "        \n",
    "        result = int_to_note[index]\n",
    "        prediction_output.append(result)\n",
    "\n",
    "        pattern.popleft()\n",
    "        pattern.append(index)\n",
    "\n",
    "    return prediction_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <div id='midiPlayerDiv60534'></div>\n",
       "                <link rel=\"stylesheet\" href=\"//cuthbertLab.github.io/music21j/css/m21.css\"\n",
       "                    type=\"text/css\" />\n",
       "                \n",
       "                <script>\n",
       "                require.config({\n",
       "                    paths: {'music21': '//cuthbertLab.github.io/music21j/src/music21'}\n",
       "                });\n",
       "                require(['music21'], function() {\n",
       "                               mp = new music21.miditools.MidiPlayer();\n",
       "                               mp.addPlayer('#midiPlayerDiv60534');\n",
       "                               mp.base64Load('data:audio/midi;base64,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');\n",
       "                        });\n",
       "                </script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'test_output.mid'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "midi_stream = create_midi(generate())\n",
    "midi_stream.show('midi')\n",
    "midi_stream.write('midi', fp='test_output.mid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus Question 8 (10 points, but your total will not exceed 100)\n",
    "\n",
    "There are many other ways in which this model could be improved for the goal of creating music that sounds like the training set. Identify two shortcomings of the model performance, and propose an idea you would use to overcome each of the shortcomings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``` Your response here ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Underfitting/Overfitting. \n",
    "    Solution: \n",
    "    to prevent underfitting, train using larger datasets; \n",
    "    to prevent overfitting, provide a validation dataset and introduce regularization.\n",
    "- Generated music has repetitive patterns.\n",
    "    Solution: use larger datasets with more diverse patterns."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
